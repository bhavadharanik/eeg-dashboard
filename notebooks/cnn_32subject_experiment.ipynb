{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# CNN Baseline Experiment - Full 32 Subject LOSO\n\n## Development of an Explainable EEG Emotion Recognition Dashboard Using CNN and CLIP Models\n\n---\n\n### Research Questions:\n- **RQ1:** How does CLIP-based transfer learning compare to CNN trained from scratch for EEG emotion classification?\n- **RQ2:** How well do these approaches generalise across different subjects?\n- **RQ3:** What features do the models learn, and do they align with known neurophysiological markers?\n\n### Research Objectives:\n- **RO1:** Implement and evaluate a CNN baseline for EEG spectrogram classification\n- **RO2:** Compare CNN vs CLIP using Leave-One-Subject-Out cross-validation\n- **RO3:** Analyse model interpretability using XAI techniques\n\n---\n\n### This Notebook:\nRuns the **CNN baseline** with full 32-subject LOSO cross-validation on the DEAP dataset.\n\n**Requirements:** Colab Pro (25GB RAM, T4/A100 GPU)  \n**Estimated Time:** 1-2 hours with T4 GPU"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Step 1: System Check { display-mode: \"form\" }\n",
    "import torch\n",
    "import psutil\n",
    "import os\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"SYSTEM INFORMATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# RAM Check\n",
    "ram_gb = psutil.virtual_memory().total / (1024**3)\n",
    "print(f\"\\nRAM: {ram_gb:.1f} GB\", end=\"\")\n",
    "if ram_gb >= 20:\n",
    "    print(\" ✓ (Colab Pro detected)\")\n",
    "else:\n",
    "    print(\" ⚠ (Consider upgrading to Colab Pro for 25GB+ RAM)\")\n",
    "\n",
    "# GPU Check\n",
    "print(f\"\\nPyTorch Version: {torch.__version__}\")\n",
    "print(f\"CUDA Available: {torch.cuda.is_available()}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    gpu_name = torch.cuda.get_device_name(0)\n",
    "    gpu_mem = torch.cuda.get_device_properties(0).total_memory / (1024**3)\n",
    "    print(f\"GPU: {gpu_name} ({gpu_mem:.1f} GB) ✓\")\n",
    "    DEVICE = torch.device('cuda')\n",
    "else:\n",
    "    print(\"\\n❌ ERROR: No GPU detected!\")\n",
    "    print(\"Go to: Runtime > Change runtime type > GPU\")\n",
    "    DEVICE = torch.device('cpu')\n",
    "\n",
    "print(f\"\\nUsing: {DEVICE}\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Step 2: Upload Data { display-mode: \"form\" }\n",
    "from google.colab import files\n",
    "import zipfile\n",
    "\n",
    "print(\"Please upload: spectrogram_cache.zip\")\n",
    "print(\"(Located at: results/spectrogram_cache.zip in your project)\\n\")\n",
    "\n",
    "uploaded = files.upload()\n",
    "\n",
    "# Extract uploaded file\n",
    "for filename in uploaded.keys():\n",
    "    if filename.endswith('.zip'):\n",
    "        print(f\"\\nExtracting {filename}...\")\n",
    "        with zipfile.ZipFile(filename, 'r') as zf:\n",
    "            zf.extractall('.')\n",
    "        print(\"Extraction complete!\")\n",
    "\n",
    "# Verify extraction\n",
    "import glob\n",
    "npz_files = glob.glob('spectrogram_cache/*.npz')\n",
    "print(f\"\\nFound {len(npz_files)} subject files ✓\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Step 3: Configuration { display-mode: \"form\" }\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "CONFIG = {\n",
    "    # Paths\n",
    "    'cache_path': 'spectrogram_cache',\n",
    "    'results_path': 'cnn_results',\n",
    "    \n",
    "    # Experiment\n",
    "    'num_subjects': 32,\n",
    "    'image_size': 224,\n",
    "    \n",
    "    # Training\n",
    "    'batch_size': 32,\n",
    "    'epochs': 15,\n",
    "    'learning_rate': 0.001,\n",
    "    'weight_decay': 0.01,\n",
    "    'early_stopping_patience': 5,\n",
    "    \n",
    "    # Reproducibility\n",
    "    'random_seed': 42,\n",
    "}\n",
    "\n",
    "# Create results directory\n",
    "os.makedirs(CONFIG['results_path'], exist_ok=True)\n",
    "\n",
    "print(\"EXPERIMENT CONFIGURATION\")\n",
    "print(\"=\" * 40)\n",
    "for key, value in CONFIG.items():\n",
    "    print(f\"{key:25s}: {value}\")\n",
    "print(\"=\" * 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Step 4: Import Libraries { display-mode: \"form\" }\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, f1_score, precision_score, recall_score,\n",
    "    balanced_accuracy_score, roc_auc_score, confusion_matrix\n",
    ")\n",
    "import gc\n",
    "\n",
    "# Set random seeds\n",
    "np.random.seed(CONFIG['random_seed'])\n",
    "torch.manual_seed(CONFIG['random_seed'])\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(CONFIG['random_seed'])\n",
    "\n",
    "print(\"All libraries imported ✓\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "#@title Step 5: Define CNN Model { display-mode: \"form\" }\n\nclass EEG_CNN(nn.Module):\n    \"\"\"\n    Lightweight CNN for EEG Spectrogram Emotion Classification.\n    \n    Architecture:\n    ================================================\n    Input: (batch, 3, 224, 224) - RGB spectrogram\n    \n    Conv Block 1: Conv2d(32) -> BatchNorm -> ReLU -> MaxPool\n        Output: (batch, 32, 112, 112)\n    \n    Conv Block 2: Conv2d(64) -> BatchNorm -> ReLU -> MaxPool\n        Output: (batch, 64, 56, 56)\n    \n    Conv Block 3: Conv2d(128) -> BatchNorm -> ReLU -> MaxPool\n        Output: (batch, 128, 28, 28)\n    \n    Global Average Pooling -> (batch, 128)\n    \n    MLP Classifier:\n        Linear(128, 256) -> ReLU -> Dropout(0.3)\n        Linear(256, 128) -> ReLU -> Dropout(0.2)\n        Linear(128, 2) -> Output logits\n    \"\"\"\n    \n    def __init__(self, num_classes=2):\n        super(EEG_CNN, self).__init__()\n        \n        # Convolutional Blocks\n        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)\n        self.bn1 = nn.BatchNorm2d(32)\n        self.pool1 = nn.MaxPool2d(2, 2)\n        \n        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n        self.bn2 = nn.BatchNorm2d(64)\n        self.pool2 = nn.MaxPool2d(2, 2)\n        \n        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n        self.bn3 = nn.BatchNorm2d(128)\n        self.pool3 = nn.MaxPool2d(2, 2)\n        \n        # Global Average Pooling\n        self.global_pool = nn.AdaptiveAvgPool2d(1)\n        \n        # MLP Classifier\n        self.fc1 = nn.Linear(128, 256)\n        self.dropout1 = nn.Dropout(0.3)\n        self.fc2 = nn.Linear(256, 128)\n        self.dropout2 = nn.Dropout(0.2)\n        self.fc3 = nn.Linear(128, num_classes)\n    \n    def forward(self, x):\n        # Conv Block 1\n        x = self.pool1(F.relu(self.bn1(self.conv1(x))))\n        # Conv Block 2\n        x = self.pool2(F.relu(self.bn2(self.conv2(x))))\n        # Conv Block 3\n        x = self.pool3(F.relu(self.bn3(self.conv3(x))))\n        # Global Pooling\n        x = self.global_pool(x)\n        x = x.view(x.size(0), -1)\n        # MLP\n        x = self.dropout1(F.relu(self.fc1(x)))\n        x = self.dropout2(F.relu(self.fc2(x)))\n        x = self.fc3(x)\n        return x\n\n# Validate model\nmodel = EEG_CNN().to(DEVICE)\ndummy = torch.randn(2, 3, 224, 224).to(DEVICE)\nout = model(dummy)\nparams = sum(p.numel() for p in model.parameters())\n\nprint(\"CNN MODEL ARCHITECTURE\")\nprint(\"=\" * 40)\nprint(f\"Input shape:  (batch, 3, 224, 224)\")\nprint(f\"Output shape: {tuple(out.shape)}\")\nprint(f\"Parameters:   {params:,}\")\nprint(\"=\" * 40)\n\ndel model, dummy, out\ntorch.cuda.empty_cache()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Step 6: Data Loading Functions { display-mode: \"form\" }\n",
    "\n",
    "def load_subject(subject_id):\n",
    "    \"\"\"\n",
    "    Load spectrogram data for a single subject.\n",
    "    \n",
    "    Returns:\n",
    "        specs: (N, C, H, W) float32 tensor normalized to [0,1]\n",
    "        valence: (N,) int64 binary labels\n",
    "        arousal: (N,) int64 binary labels\n",
    "    \"\"\"\n",
    "    path = f\"{CONFIG['cache_path']}/s{subject_id:02d}_spectrograms.npz\"\n",
    "    \n",
    "    if not os.path.exists(path):\n",
    "        return None, None, None\n",
    "    \n",
    "    data = np.load(path)\n",
    "    specs = data['spectrograms'].astype(np.float32) / 255.0\n",
    "    valence = data['valence'].astype(np.int64)\n",
    "    arousal = data['arousal'].astype(np.int64)\n",
    "    data.close()\n",
    "    \n",
    "    # Convert (N, H, W, C) -> (N, C, H, W)\n",
    "    specs = np.transpose(specs, (0, 3, 1, 2))\n",
    "    \n",
    "    return specs, valence, arousal\n",
    "\n",
    "\n",
    "# Discover available subjects\n",
    "available_subjects = []\n",
    "for s in range(1, CONFIG['num_subjects'] + 1):\n",
    "    path = f\"{CONFIG['cache_path']}/s{s:02d}_spectrograms.npz\"\n",
    "    if os.path.exists(path):\n",
    "        available_subjects.append(s)\n",
    "\n",
    "print(f\"Available subjects: {len(available_subjects)}/{CONFIG['num_subjects']}\")\n",
    "print(f\"Subject IDs: {available_subjects}\")\n",
    "\n",
    "# Test loading one subject\n",
    "test_specs, test_v, test_a = load_subject(available_subjects[0])\n",
    "print(f\"\\nSample data shape: {test_specs.shape}\")\n",
    "print(f\"Valence labels: {np.bincount(test_v)} (neg/pos)\")\n",
    "print(f\"Arousal labels: {np.bincount(test_a)} (neg/pos)\")\n",
    "del test_specs, test_v, test_a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Step 7: Training Function { display-mode: \"form\" }\n",
    "\n",
    "def train_loso_fold(train_subjects, test_subject, label_key, device=DEVICE):\n",
    "    \"\"\"\n",
    "    Train CNN for one Leave-One-Subject-Out fold.\n",
    "    \n",
    "    Args:\n",
    "        train_subjects: List of subject IDs for training\n",
    "        test_subject: Subject ID for testing (held out)\n",
    "        label_key: 'valence' or 'arousal'\n",
    "        device: torch device\n",
    "    \n",
    "    Returns:\n",
    "        metrics: Dictionary with all evaluation metrics\n",
    "    \"\"\"\n",
    "    \n",
    "    # === Load Training Data ===\n",
    "    X_train_list, y_train_list = [], []\n",
    "    \n",
    "    for subj in train_subjects:\n",
    "        specs, valence, arousal = load_subject(subj)\n",
    "        if specs is not None:\n",
    "            X_train_list.append(specs)\n",
    "            y_train_list.append(valence if label_key == 'valence' else arousal)\n",
    "    \n",
    "    X_train = np.concatenate(X_train_list, axis=0)\n",
    "    y_train = np.concatenate(y_train_list, axis=0)\n",
    "    del X_train_list, y_train_list\n",
    "    \n",
    "    # === Load Test Data ===\n",
    "    X_test, valence, arousal = load_subject(test_subject)\n",
    "    y_test = valence if label_key == 'valence' else arousal\n",
    "    \n",
    "    # === Create DataLoaders ===\n",
    "    train_dataset = TensorDataset(\n",
    "        torch.FloatTensor(X_train),\n",
    "        torch.LongTensor(y_train)\n",
    "    )\n",
    "    test_dataset = TensorDataset(\n",
    "        torch.FloatTensor(X_test),\n",
    "        torch.LongTensor(y_test)\n",
    "    )\n",
    "    \n",
    "    train_loader = DataLoader(\n",
    "        train_dataset, \n",
    "        batch_size=CONFIG['batch_size'], \n",
    "        shuffle=True,\n",
    "        num_workers=2,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    test_loader = DataLoader(\n",
    "        test_dataset, \n",
    "        batch_size=CONFIG['batch_size'],\n",
    "        num_workers=2,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    \n",
    "    # Free memory\n",
    "    n_train, n_test = len(X_train), len(X_test)\n",
    "    pos_rate = float(np.mean(y_test))\n",
    "    del X_train, X_test\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    # === Initialize Model ===\n",
    "    model = EEG_CNN(num_classes=2).to(device)\n",
    "    \n",
    "    # Class weights for imbalanced data\n",
    "    class_counts = np.bincount(y_train, minlength=2)\n",
    "    class_weights = torch.FloatTensor(\n",
    "        [len(y_train) / (2 * max(c, 1)) for c in class_counts]\n",
    "    ).to(device)\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "    optimizer = optim.Adam(\n",
    "        model.parameters(), \n",
    "        lr=CONFIG['learning_rate'], \n",
    "        weight_decay=CONFIG['weight_decay']\n",
    "    )\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, mode='min', patience=2, factor=0.5\n",
    "    )\n",
    "    \n",
    "    # === Training Loop ===\n",
    "    best_loss = float('inf')\n",
    "    best_state = None\n",
    "    patience_counter = 0\n",
    "    \n",
    "    for epoch in range(CONFIG['epochs']):\n",
    "        model.train()\n",
    "        epoch_loss = 0.0\n",
    "        \n",
    "        for batch_X, batch_y in train_loader:\n",
    "            batch_X = batch_X.to(device)\n",
    "            batch_y = batch_y.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(batch_X)\n",
    "            loss = criterion(outputs, batch_y)\n",
    "            loss.backward()\n",
    "            \n",
    "            # Gradient clipping\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            \n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "        \n",
    "        avg_loss = epoch_loss / len(train_loader)\n",
    "        scheduler.step(avg_loss)\n",
    "        \n",
    "        # Early stopping check\n",
    "        if avg_loss < best_loss:\n",
    "            best_loss = avg_loss\n",
    "            best_state = {k: v.cpu().clone() for k, v in model.state_dict().items()}\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= CONFIG['early_stopping_patience']:\n",
    "                break\n",
    "    \n",
    "    # Load best model\n",
    "    if best_state is not None:\n",
    "        model.load_state_dict(best_state)\n",
    "    \n",
    "    # === Evaluation ===\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_probs = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_X, batch_y in test_loader:\n",
    "            batch_X = batch_X.to(device)\n",
    "            outputs = model(batch_X)\n",
    "            probs = F.softmax(outputs, dim=1)\n",
    "            preds = torch.argmax(probs, dim=1)\n",
    "            \n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_probs.extend(probs[:, 1].cpu().numpy())\n",
    "            all_labels.extend(batch_y.numpy())\n",
    "    \n",
    "    y_true = np.array(all_labels)\n",
    "    y_pred = np.array(all_preds)\n",
    "    y_prob = np.array(all_probs)\n",
    "    \n",
    "    # === Calculate Metrics ===\n",
    "    metrics = {\n",
    "        'accuracy': float(accuracy_score(y_true, y_pred)),\n",
    "        'f1_score': float(f1_score(y_true, y_pred, zero_division=0)),\n",
    "        'precision': float(precision_score(y_true, y_pred, zero_division=0)),\n",
    "        'recall': float(recall_score(y_true, y_pred, zero_division=0)),\n",
    "        'balanced_accuracy': float(balanced_accuracy_score(y_true, y_pred)),\n",
    "        'n_train': n_train,\n",
    "        'n_test': n_test,\n",
    "        'pos_rate': pos_rate,\n",
    "    }\n",
    "    \n",
    "    # ROC-AUC (handle edge cases)\n",
    "    try:\n",
    "        if len(np.unique(y_true)) > 1:\n",
    "            metrics['roc_auc'] = float(roc_auc_score(y_true, y_prob))\n",
    "        else:\n",
    "            metrics['roc_auc'] = 0.5\n",
    "    except:\n",
    "        metrics['roc_auc'] = 0.5\n",
    "    \n",
    "    # === Cleanup ===\n",
    "    del model, train_loader, test_loader, train_dataset, test_dataset\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "print(\"Training function defined ✓\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Step 8: Checkpoint Functions { display-mode: \"form\" }\n",
    "\n",
    "def save_checkpoint(results):\n",
    "    \"\"\"Save results checkpoint after each fold.\"\"\"\n",
    "    path = f\"{CONFIG['results_path']}/checkpoint.json\"\n",
    "    with open(path, 'w') as f:\n",
    "        json.dump(results, f, indent=2)\n",
    "\n",
    "def load_checkpoint():\n",
    "    \"\"\"Load existing checkpoint if available.\"\"\"\n",
    "    path = f\"{CONFIG['results_path']}/checkpoint.json\"\n",
    "    if os.path.exists(path):\n",
    "        with open(path, 'r') as f:\n",
    "            return json.load(f)\n",
    "    return None\n",
    "\n",
    "print(\"Checkpoint functions ready ✓\")\n",
    "print(\"Results will be saved after each subject to prevent data loss.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Step 9: Run Complete LOSO Experiment { display-mode: \"form\" }\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"CNN BASELINE EXPERIMENT - FULL 32-SUBJECT LOSO CROSS-VALIDATION\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"\\nStarted: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(f\"Device:  {DEVICE}\")\n",
    "print(f\"Subjects: {len(available_subjects)}\")\n",
    "\n",
    "# Initialize or load results\n",
    "checkpoint = load_checkpoint()\n",
    "\n",
    "if checkpoint:\n",
    "    results = checkpoint\n",
    "    print(\"\\n✓ Resuming from checkpoint\")\n",
    "else:\n",
    "    results = {\n",
    "        'valence': {'per_subject': {}},\n",
    "        'arousal': {'per_subject': {}},\n",
    "        'config': CONFIG,\n",
    "        'model': 'EEG_CNN',\n",
    "        'architecture': {\n",
    "            'type': '3 Conv Blocks + Global Avg Pool + MLP',\n",
    "            'conv_channels': [32, 64, 128],\n",
    "            'mlp_dims': [128, 256, 128, 2],\n",
    "            'dropout': [0.3, 0.2]\n",
    "        },\n",
    "        'started': datetime.now().isoformat()\n",
    "    }\n",
    "    print(\"\\n✓ Starting fresh experiment\")\n",
    "\n",
    "# Run experiment for each task\n",
    "for task in ['valence', 'arousal']:\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"TASK: {task.upper()}\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    completed = set(results[task]['per_subject'].keys())\n",
    "    print(f\"Completed: {len(completed)}/{len(available_subjects)} subjects\")\n",
    "    \n",
    "    for idx, test_subj in enumerate(available_subjects):\n",
    "        subj_key = str(test_subj)\n",
    "        \n",
    "        # Skip if already completed\n",
    "        if subj_key in completed:\n",
    "            m = results[task]['per_subject'][subj_key]\n",
    "            print(f\"[{idx+1:2d}/{len(available_subjects)}] S{test_subj:02d}: DONE (F1={m['f1_score']:.3f})\")\n",
    "            continue\n",
    "        \n",
    "        print(f\"\\n[{idx+1:2d}/{len(available_subjects)}] S{test_subj:02d}: Training...\", end=\" \")\n",
    "        \n",
    "        # Get training subjects (all except test)\n",
    "        train_subjs = [s for s in available_subjects if s != test_subj]\n",
    "        \n",
    "        # Train and evaluate\n",
    "        metrics = train_loso_fold(train_subjs, test_subj, task)\n",
    "        \n",
    "        # Store results\n",
    "        results[task]['per_subject'][subj_key] = metrics\n",
    "        \n",
    "        # Save checkpoint\n",
    "        save_checkpoint(results)\n",
    "        \n",
    "        # Print results\n",
    "        print(f\"F1={metrics['f1_score']:.3f}, Acc={metrics['accuracy']:.3f}, \"\n",
    "              f\"BAcc={metrics['balanced_accuracy']:.3f}, Pos={metrics['pos_rate']:.1%} [Saved]\")\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"ALL FOLDS COMPLETE!\")\n",
    "print(f\"{'='*70}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Step 10: Calculate Summary Statistics { display-mode: \"form\" }\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"CALCULATING SUMMARY STATISTICS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for task in ['valence', 'arousal']:\n",
    "    per_subject = results[task]['per_subject']\n",
    "    \n",
    "    if not per_subject:\n",
    "        continue\n",
    "    \n",
    "    metrics_list = list(per_subject.values())\n",
    "    n = len(metrics_list)\n",
    "    \n",
    "    # Calculate all summary statistics\n",
    "    results[task]['summary'] = {\n",
    "        # F1 Score\n",
    "        'mean_f1': float(np.mean([m['f1_score'] for m in metrics_list])),\n",
    "        'std_f1': float(np.std([m['f1_score'] for m in metrics_list])),\n",
    "        'min_f1': float(np.min([m['f1_score'] for m in metrics_list])),\n",
    "        'max_f1': float(np.max([m['f1_score'] for m in metrics_list])),\n",
    "        \n",
    "        # Accuracy\n",
    "        'mean_accuracy': float(np.mean([m['accuracy'] for m in metrics_list])),\n",
    "        'std_accuracy': float(np.std([m['accuracy'] for m in metrics_list])),\n",
    "        \n",
    "        # Balanced Accuracy\n",
    "        'mean_balanced_accuracy': float(np.mean([m['balanced_accuracy'] for m in metrics_list])),\n",
    "        'std_balanced_accuracy': float(np.std([m['balanced_accuracy'] for m in metrics_list])),\n",
    "        \n",
    "        # Precision & Recall\n",
    "        'mean_precision': float(np.mean([m['precision'] for m in metrics_list])),\n",
    "        'std_precision': float(np.std([m['precision'] for m in metrics_list])),\n",
    "        'mean_recall': float(np.mean([m['recall'] for m in metrics_list])),\n",
    "        'std_recall': float(np.std([m['recall'] for m in metrics_list])),\n",
    "        \n",
    "        # ROC-AUC\n",
    "        'mean_roc_auc': float(np.mean([m['roc_auc'] for m in metrics_list])),\n",
    "        'std_roc_auc': float(np.std([m['roc_auc'] for m in metrics_list])),\n",
    "        \n",
    "        # Metadata\n",
    "        'num_subjects': n,\n",
    "        'total_test_samples': sum(m['n_test'] for m in metrics_list),\n",
    "    }\n",
    "    \n",
    "    # Print summary\n",
    "    s = results[task]['summary']\n",
    "    print(f\"\\n{task.upper()} ({n} subjects):\")\n",
    "    print(f\"  F1 Score:       {s['mean_f1']:.3f} ± {s['std_f1']:.3f}  [{s['min_f1']:.3f} - {s['max_f1']:.3f}]\")\n",
    "    print(f\"  Accuracy:       {s['mean_accuracy']:.3f} ± {s['std_accuracy']:.3f}\")\n",
    "    print(f\"  Balanced Acc:   {s['mean_balanced_accuracy']:.3f} ± {s['std_balanced_accuracy']:.3f}\")\n",
    "    print(f\"  Precision:      {s['mean_precision']:.3f} ± {s['std_precision']:.3f}\")\n",
    "    print(f\"  Recall:         {s['mean_recall']:.3f} ± {s['std_recall']:.3f}\")\n",
    "    print(f\"  ROC-AUC:        {s['mean_roc_auc']:.3f} ± {s['std_roc_auc']:.3f}\")\n",
    "\n",
    "# Add completion timestamp\n",
    "results['completed'] = datetime.now().isoformat()\n",
    "\n",
    "# Save final results\n",
    "final_path = f\"{CONFIG['results_path']}/cnn_baseline_results.json\"\n",
    "with open(final_path, 'w') as f:\n",
    "    json.dump(results, f, indent=2)\n",
    "\n",
    "print(f\"\\n✓ Results saved to: {final_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "#@title Step 11: Results Table { display-mode: \"form\" }\n\nprint(\"\\n\" + \"=\" * 70)\nprint(\"RESULTS - CNN BASELINE PERFORMANCE\")\nprint(\"=\" * 70)\n\nv = results['valence']['summary']\na = results['arousal']['summary']\n\nprint(\"\\n┌─────────────────────┬──────────────────────┬──────────────────────┐\")\nprint(\"│ Metric              │ Valence              │ Arousal              │\")\nprint(\"├─────────────────────┼──────────────────────┼──────────────────────┤\")\nprint(f\"│ F1 Score            │ {v['mean_f1']:.3f} ± {v['std_f1']:.3f}          │ {a['mean_f1']:.3f} ± {a['std_f1']:.3f}          │\")\nprint(f\"│ Accuracy            │ {v['mean_accuracy']:.3f} ± {v['std_accuracy']:.3f}          │ {a['mean_accuracy']:.3f} ± {a['std_accuracy']:.3f}          │\")\nprint(f\"│ Balanced Accuracy   │ {v['mean_balanced_accuracy']:.3f}                │ {a['mean_balanced_accuracy']:.3f}                │\")\nprint(f\"│ Precision           │ {v['mean_precision']:.3f} ± {v['std_precision']:.3f}          │ {a['mean_precision']:.3f} ± {a['std_precision']:.3f}          │\")\nprint(f\"│ Recall              │ {v['mean_recall']:.3f} ± {v['std_recall']:.3f}          │ {a['mean_recall']:.3f} ± {a['std_recall']:.3f}          │\")\nprint(f\"│ ROC-AUC             │ {v['mean_roc_auc']:.3f}                │ {a['mean_roc_auc']:.3f}                │\")\nprint(f\"│ Min F1              │ {v['min_f1']:.3f}                │ {a['min_f1']:.3f}                │\")\nprint(f\"│ Max F1              │ {v['max_f1']:.3f}                │ {a['max_f1']:.3f}                │\")\nprint(\"└─────────────────────┴──────────────────────┴──────────────────────┘\")\n\nprint(f\"\\nEvaluation: Leave-One-Subject-Out (LOSO) Cross-Validation\")\nprint(f\"Subjects: {v['num_subjects']}\")\nprint(f\"Total test samples: {v['total_test_samples']:,}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Step 12: Download Results { display-mode: \"form\" }\n",
    "from google.colab import files\n",
    "import shutil\n",
    "\n",
    "# Create zip archive\n",
    "zip_name = 'cnn_baseline_results'\n",
    "shutil.make_archive(zip_name, 'zip', CONFIG['results_path'])\n",
    "\n",
    "print(\"DOWNLOAD INSTRUCTIONS\")\n",
    "print(\"=\" * 50)\n",
    "print(\"\\n1. Click the download link below\")\n",
    "print(\"2. Extract the ZIP file\")\n",
    "print(\"3. Copy 'cnn_baseline_results.json' to:\")\n",
    "print(\"   results/cnn_baseline_experiment/\\n\")\n",
    "\n",
    "# Trigger download\n",
    "files.download(f'{zip_name}.zip')\n",
    "\n",
    "print(\"\\n✓ Experiment complete!\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}